{
  "pipelineSpec": {
    "components": {
      "comp-endpoint-create": {
        "executorLabel": "exec-endpoint-create",
        "inputDefinitions": {
          "parameters": {
            "description": {
              "type": "STRING"
            },
            "display_name": {
              "type": "STRING"
            },
            "encryption_spec_key_name": {
              "type": "STRING"
            },
            "labels": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "endpoint": {
              "artifactType": {
                "schemaTitle": "google.VertexEndpoint",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-inference-dataset": {
        "executorLabel": "exec-inference-dataset",
        "inputDefinitions": {
          "parameters": {
            "data_root": {
              "type": "STRING"
            },
            "movies_output_filename": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "seq_length": {
              "type": "INT"
            }
          }
        }
      },
      "comp-model-deploy": {
        "executorLabel": "exec-model-deploy",
        "inputDefinitions": {
          "artifacts": {
            "endpoint": {
              "artifactType": {
                "schemaTitle": "google.VertexEndpoint",
                "schemaVersion": "0.0.1"
              }
            },
            "model": {
              "artifactType": {
                "schemaTitle": "google.VertexModel",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "automatic_resources_max_replica_count": {
              "type": "INT"
            },
            "automatic_resources_min_replica_count": {
              "type": "INT"
            },
            "dedicated_resources_accelerator_count": {
              "type": "INT"
            },
            "dedicated_resources_accelerator_type": {
              "type": "STRING"
            },
            "dedicated_resources_machine_type": {
              "type": "STRING"
            },
            "dedicated_resources_max_replica_count": {
              "type": "INT"
            },
            "dedicated_resources_min_replica_count": {
              "type": "INT"
            },
            "deployed_model_display_name": {
              "type": "STRING"
            },
            "disable_container_logging": {
              "type": "STRING"
            },
            "enable_access_logging": {
              "type": "STRING"
            },
            "explanation_metadata": {
              "type": "STRING"
            },
            "explanation_parameters": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "traffic_split": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-model-upload": {
        "executorLabel": "exec-model-upload",
        "inputDefinitions": {
          "parameters": {
            "artifact_uri": {
              "type": "STRING"
            },
            "description": {
              "type": "STRING"
            },
            "display_name": {
              "type": "STRING"
            },
            "encryption_spec_key_name": {
              "type": "STRING"
            },
            "explanation_metadata": {
              "type": "STRING"
            },
            "explanation_parameters": {
              "type": "STRING"
            },
            "instance_schema_uri": {
              "type": "STRING"
            },
            "labels": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "parameters_schema_uri": {
              "type": "STRING"
            },
            "prediction_schema_uri": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "serving_container_args": {
              "type": "STRING"
            },
            "serving_container_command": {
              "type": "STRING"
            },
            "serving_container_environment_variables": {
              "type": "STRING"
            },
            "serving_container_health_route": {
              "type": "STRING"
            },
            "serving_container_image_uri": {
              "type": "STRING"
            },
            "serving_container_ports": {
              "type": "STRING"
            },
            "serving_container_predict_route": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "google.VertexModel",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-movies-dataset": {
        "executorLabel": "exec-movies-dataset",
        "inputDefinitions": {
          "parameters": {
            "data_root": {
              "type": "STRING"
            },
            "movies_output_filename": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-train-dataset": {
        "executorLabel": "exec-train-dataset",
        "inputDefinitions": {
          "parameters": {
            "data_root": {
              "type": "STRING"
            },
            "movies_output_filename": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            },
            "seq_length": {
              "type": "INT"
            }
          }
        }
      },
      "comp-train-tensorflow-model": {
        "executorLabel": "exec-train-tensorflow-model",
        "inputDefinitions": {
          "parameters": {
            "artifact_store": {
              "type": "STRING"
            },
            "base_output_directory": {
              "type": "STRING"
            },
            "bucket_name": {
              "type": "STRING"
            },
            "data_root": {
              "type": "STRING"
            },
            "inference_output_filename": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "movies_output_filename": {
              "type": "STRING"
            },
            "network": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            },
            "service_account": {
              "type": "STRING"
            },
            "tensorboard": {
              "type": "STRING"
            },
            "timestamp": {
              "type": "STRING"
            },
            "train_output_filename": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "gcp_resources": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-endpoint-create": {
          "container": {
            "args": [
              "--type",
              "CreateEndpoint",
              "--payload",
              "{\"display_name\": \"{{$.inputs.parameters['display_name']}}\", \"description\": \"{{$.inputs.parameters['description']}}\", \"labels\": {{$.inputs.parameters['labels']}}, \"encryption_spec\": {\"kms_key_name\":\"{{$.inputs.parameters['encryption_spec_key_name']}}\"}, \"network\": \"{{$.inputs.parameters['network']}}\"}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}",
              "--executor_input",
              "{{$}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.1"
          }
        },
        "exec-inference-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "inference_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.34.2' 'tensorflow==2.6.2' 'pandas==1.3.2' 'pyarrow==7.0.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef inference_dataset(\n        project_id: str,\n        data_root: str,\n        movies_output_filename: str,\n        seq_length: int,\n):\n    import os\n    from google.cloud import bigquery\n    import pandas as pd\n    import tensorflow as tf\n\n    name_transformation = \"inference\"\n\n    sql_query = f\"\"\"WITH ratings AS (\n        SELECT\n            ratings.user_id,\n            ratings.movie_id,\n            ratings.rating_id,\n            ratings.rating_timestamp_utc,\n            ratings.rating_score,\n            COALESCE(ratings.user_eligible_for_trial, False) AS user_eligible_for_trial,\n            COALESCE(ratings.user_has_payment_method, False) AS user_has_payment_method,\n            COALESCE(ratings.user_subscriber, False) AS user_subscriber,\n            COALESCE(ratings.user_trialist, False) AS user_trialist,\n            movies.movie_title,\n            COALESCE(ratings.rating_score,\n             PERCENTILE_DISC(ratings.rating_score, 0.5) OVER (PARTITION BY ratings.movie_id)) AS rating_value,\n            COALESCE(DATE_DIFF(ratings.rating_timestamp_utc, LAG(ratings.rating_timestamp_utc, 1)\n                OVER (PARTITION BY user_id ORDER BY ratings.rating_timestamp_utc),  DAY), -1) AS days_since_last_rating,\n            COALESCE(movies.movie_release_year, 0) as movie_release_year,\n            movies.movie_title_language,\n            LAST_VALUE(ratings.rating_timestamp_utc) OVER (PARTITION BY user_id\n                ORDER BY ratings.rating_timestamp_utc\n                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_rating_timestamp\n        FROM `__DATASET_ID__.mubi_ratings_data` ratings\n        JOIN `__DATASET_ID__.mubi_movie_data` movies ON\n            ratings.movie_id = movies.movie_id\n            AND ratings.rating_score IS NOT NULL\n    ),\n    shifted_last_rating AS (\n         SELECT\n             ratings.user_id,\n             ratings.movie_id,\n             COALESCE(LAG(ratings.days_since_last_rating, 1) over (PARTITION BY user_id\n                ORDER BY ratings.days_since_last_rating DESC), 0) as shifted_days,\n         FROM ratings\n    ),\n    sequenced_rating AS (\n        SELECT\n            movie_title,\n            ratings.movie_id,\n            ratings.user_id,\n            rating_value,\n            movie_release_year,\n            rating_timestamp_utc,\n            user_eligible_for_trial,\n            days_since_last_rating,\n            user_has_payment_method,\n            user_subscriber,\n            user_trialist,\n            last_rating_timestamp,\n            ARRAY_AGG(ratings.movie_id) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_movie_ids,\n            ARRAY_AGG(movie_release_year) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_movie_years,\n            ARRAY_AGG(rating_value) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_score,\n            ARRAY_AGG(shifted_days) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_days_since_last_rating,\n        FROM ratings\n        INNER JOIN shifted_last_rating ON\n            ratings.user_id = shifted_last_rating.user_id\n            AND ratings.movie_id = shifted_last_rating.movie_id\n    )\n    SELECT * FROM sequenced_rating\n    WHERE ARRAY_LENGTH(previous_movie_ids) > 2\n        AND rating_timestamp_utc = last_rating_timestamp\"\"\"\n\n\n    data_path = os.path.join(data_root, name_transformation)\n\n    output_path = os.path.join(data_path, movies_output_filename)\n\n    def load_dataset(query) -> pd.DataFrame:\n        bq_client = bigquery.Client()\n        results = bq_client.query(query, project=project_id).to_dataframe()\n        return results\n\n    def save_tf_dataset(dict_features) -> None:\n        dataset = tf.data.Dataset.from_tensor_slices(dict_features)\n\n        if not tf.io.gfile.exists(data_path):\n            tf.io.gfile.makedirs(data_path)\n\n        tf.data.experimental.save(dataset, output_path)\n\n    def get_features_dict(rows) -> dict:\n        dict_features = dict(\n            **rows[[\"user_id\"]].astype(\"int\"),\n            **rows[[\"user_eligible_for_trial\"]].astype(\"int\"),\n            **rows[[\"user_has_payment_method\"]].astype(\"int\"),\n            **rows[[\"user_subscriber\"]].astype(\"int\"),\n            **rows[[\"user_trialist\"]].astype(\"int\"),\n            **{\"previous_movie_ids\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_movie_ids\"].values,\n                                                              maxlen=seq_length, dtype='int32', value=0)},\n            **{\"previous_movie_years\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_movie_years\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=1980.0)},\n            **{\"previous_score\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_score\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=2.5)},\n            **{\"previous_days_since_last_rating\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_days_since_last_rating\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=0)}\n        )\n        return dict_features\n\n    try:\n        assert not tf.io.gfile.exists(output_path)\n    except AssertionError:\n        raise ValueError(\"Dataset already exists, load it. Remove timestamp from config in case of new run\")\n\n    sql_query = (\n        sql_query\n            .replace(\"__DATASET_ID__\", str(\"mubi_movie_data\"))\n            .replace(\"__FRAME_START__\", str(9))\n            .replace(\"__FRAME_END__\", str(0))\n    )\n\n    rows = load_dataset(sql_query)\n\n    dict_features = get_features_dict(rows)\n    del rows\n\n    save_tf_dataset(dict_features)\n\n"
            ],
            "image": "python:3.8"
          }
        },
        "exec-model-deploy": {
          "container": {
            "args": [
              "--type",
              "DeployModel",
              "--payload",
              "{\"endpoint\": \"{{$.inputs.artifacts['endpoint'].metadata['resourceName']}}\", \"traffic_split\": {{$.inputs.parameters['traffic_split']}}, \"deployed_model\": {\"model\": \"{{$.inputs.artifacts['model'].metadata['resourceName']}}\", \"dedicated_resources\": {\"machine_spec\": {\"machine_type\": \"{{$.inputs.parameters['dedicated_resources_machine_type']}}\", \"accelerator_type\": \"{{$.inputs.parameters['dedicated_resources_accelerator_type']}}\", \"accelerator_count\": {{$.inputs.parameters['dedicated_resources_accelerator_count']}}}, \"min_replica_count\": {{$.inputs.parameters['dedicated_resources_min_replica_count']}}, \"max_replica_count\": {{$.inputs.parameters['dedicated_resources_max_replica_count']}}}, \"automatic_resources\": {\"min_replica_count\": {{$.inputs.parameters['automatic_resources_min_replica_count']}}, \"max_replica_count\": {{$.inputs.parameters['automatic_resources_max_replica_count']}}}, \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"disable_container_logging\": {{$.inputs.parameters['disable_container_logging']}}, \"enable_access_logging\": {{$.inputs.parameters['enable_access_logging']}}, \"explanation_spec\": {\"parameters\": {{$.inputs.parameters['explanation_parameters']}}, \"metadata\": {{$.inputs.parameters['explanation_metadata']}}}}}",
              "--project",
              "",
              "--location",
              "",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.1"
          }
        },
        "exec-model-upload": {
          "container": {
            "args": [
              "--type",
              "UploadModel",
              "--payload",
              "{\"display_name\": \"{{$.inputs.parameters['display_name']}}\", \"description\": \"{{$.inputs.parameters['description']}}\", \"predict_schemata\": {\"instance_schema_uri\": \"{{$.inputs.parameters['instance_schema_uri']}}\", \"parameters_schema_uri\": \"{{$.inputs.parameters['parameters_schema_uri']}}\", \"prediction_schema_uri\": \"{{$.inputs.parameters['prediction_schema_uri']}}\"}, \"container_spec\": {\"image_uri\": \"{{$.inputs.parameters['serving_container_image_uri']}}\", \"command\": {{$.inputs.parameters['serving_container_command']}}, \"args\": {{$.inputs.parameters['serving_container_args']}}, \"env\": {{$.inputs.parameters['serving_container_environment_variables']}}, \"ports\": {{$.inputs.parameters['serving_container_ports']}}, \"predict_route\": \"{{$.inputs.parameters['serving_container_predict_route']}}\", \"health_route\": \"{{$.inputs.parameters['serving_container_health_route']}}\"}, \"artifact_uri\": \"{{$.inputs.parameters['artifact_uri']}}\", \"explanation_spec\": {\"parameters\": {{$.inputs.parameters['explanation_parameters']}}, \"metadata\": {{$.inputs.parameters['explanation_metadata']}}}, \"encryption_spec\": {\"kms_key_name\":\"{{$.inputs.parameters['encryption_spec_key_name']}}\"}, \"labels\": {{$.inputs.parameters['labels']}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}",
              "--executor_input",
              "{{$}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.1"
          }
        },
        "exec-movies-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "movies_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.34.2' 'tensorflow==2.6.2' 'pandas==1.3.2' 'pyarrow==7.0.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef movies_dataset(\n        project_id: str,\n        data_root: str,\n        movies_output_filename: str,\n):\n    import os\n    from google.cloud import bigquery\n    import pandas as pd\n    import tensorflow as tf\n\n    name_transformation = \"movies\"\n\n    sql_query = f\"\"\"SELECT DISTINCT\n            ratings.movie_id,\n            movies.movie_title,\n        FROM `mubi_movie_data.mubi_ratings_data` ratings\n        JOIN `mubi_movie_data.mubi_movie_data` movies ON\n            ratings.movie_id = movies.movie_id\"\"\"\n\n    data_path = os.path.join(data_root, name_transformation)\n\n    output_path = os.path.join(data_path, movies_output_filename)\n\n    def load_dataset(query) -> pd.DataFrame:\n        bq_client = bigquery.Client()\n        results = bq_client.query(query, project=project_id).to_dataframe()\n        return results\n\n    def save_tf_dataset(dict_features) -> None:\n        dataset = tf.data.Dataset.from_tensor_slices(dict_features)\n\n        if not tf.io.gfile.exists(data_path):\n            tf.io.gfile.makedirs(data_path)\n\n        tf.data.experimental.save(dataset, output_path)\n\n    def get_features_dict(rows) -> dict:\n        dict_features = dict(\n            **rows[[\"movie_id\"]].astype(\"int\"),\n            **rows[[\"movie_title\"]].astype(\"str\"),\n        )\n        return dict_features\n\n    try:\n        assert not tf.io.gfile.exists(output_path)\n    except AssertionError:\n        raise ValueError(\"Dataset already exists, load it. Remove timestamp from config in case of new run\")\n\n    rows = load_dataset(sql_query)\n\n    dict_features = get_features_dict(rows)\n    del rows\n\n    save_tf_dataset(dict_features)\n\n"
            ],
            "image": "python:3.8"
          }
        },
        "exec-train-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-bigquery==2.34.2' 'tensorflow==2.6.2' 'pandas==1.3.2' 'pyarrow==7.0.0' 'kfp==1.8.10' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_dataset(\n        project_id: str,\n        data_root: str,\n        movies_output_filename: str,\n        seq_length: int,\n):\n    import os\n    from google.cloud import bigquery\n    import pandas as pd\n    import tensorflow as tf\n\n    name_transformation = \"train\"\n\n    sql_query = \"\"\"WITH ratings AS (\n        SELECT\n            ratings.user_id,\n            ratings.movie_id,\n            ratings.rating_id,\n            ratings.rating_timestamp_utc,\n            ratings.rating_score,\n            COALESCE(ratings.user_eligible_for_trial, False) AS user_eligible_for_trial,\n            COALESCE(ratings.user_has_payment_method, False) AS user_has_payment_method,\n            COALESCE(ratings.user_subscriber, False) AS user_subscriber,\n            COALESCE(ratings.user_trialist, False) AS user_trialist,\n            movies.movie_title,\n            COALESCE(ratings.rating_score,\n             PERCENTILE_DISC(ratings.rating_score, 0.5) OVER (PARTITION BY ratings.movie_id)) AS rating_value,\n            COALESCE(DATE_DIFF(ratings.rating_timestamp_utc, LAG(ratings.rating_timestamp_utc, 1)\n                OVER (PARTITION BY user_id ORDER BY ratings.rating_timestamp_utc),  DAY), -1) AS days_since_last_rating,\n            COALESCE(movies.movie_release_year, 0) as movie_release_year,\n            movies.movie_title_language,\n            LAST_VALUE(ratings.rating_timestamp_utc) OVER (PARTITION BY user_id\n                ORDER BY ratings.rating_timestamp_utc\n                ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_rating_timestamp\n        FROM `__DATASET_ID__.mubi_ratings_data` ratings\n        JOIN `__DATASET_ID__.mubi_movie_data` movies ON\n            ratings.movie_id = movies.movie_id\n            AND ratings.rating_score IS NOT NULL\n    ),\n    shifted_last_rating AS (\n         SELECT\n             ratings.user_id,\n             ratings.movie_id,\n             COALESCE(LAG(ratings.days_since_last_rating, 1) over (PARTITION BY user_id\n                ORDER BY ratings.days_since_last_rating DESC), 0) as shifted_days,\n         FROM ratings\n    ),\n    sequenced_rating AS (\n        SELECT\n            movie_title,\n            ratings.movie_id,\n            ratings.user_id,\n            rating_value,\n            movie_release_year,\n            rating_timestamp_utc,\n            user_eligible_for_trial,\n            days_since_last_rating,\n            user_has_payment_method,\n            user_subscriber,\n            user_trialist,\n            last_rating_timestamp,\n            ARRAY_AGG(ratings.movie_id) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_movie_ids,\n            ARRAY_AGG(movie_release_year) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_movie_years,\n            ARRAY_AGG(rating_value) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_score,\n            ARRAY_AGG(shifted_days) OVER (PARTITION BY ratings.user_id\n                ORDER BY rating_timestamp_utc asc\n                ROWS BETWEEN __FRAME_START__ PRECEDING AND __FRAME_END__ PRECEDING) AS previous_days_since_last_rating,\n        FROM ratings\n        INNER JOIN shifted_last_rating ON\n            ratings.user_id = shifted_last_rating.user_id\n            AND ratings.movie_id = shifted_last_rating.movie_id\n    )\n    SELECT * FROM sequenced_rating --where ARRAY_LENGTH(previous_movie_ids) > 2\n    WHERE ABS(MOD(FARM_FINGERPRINT(CAST(rating_timestamp_utc AS STRING)), 100)) IN (10, 20, 30, 40)\"\"\"\n\n    data_path = os.path.join(data_root, name_transformation)\n\n    output_path = os.path.join(data_path, movies_output_filename)\n\n    def load_dataset(query) -> pd.DataFrame:\n        bq_client = bigquery.Client()\n        results = bq_client.query(query, project=project_id).to_dataframe()\n        return results\n\n    def save_tf_dataset(dict_features) -> None:\n        dataset = tf.data.Dataset.from_tensor_slices(dict_features)\n\n        if not tf.io.gfile.exists(data_path):\n            tf.io.gfile.makedirs(data_path)\n\n        tf.data.experimental.save(dataset, output_path)\n\n    def get_features_dict(rows) -> dict:\n        dict_features = dict(\n            **rows[[\"movie_id\"]].astype(\"int\"),\n            **rows[[\"user_eligible_for_trial\"]].astype(\"int\"),\n            **rows[[\"user_has_payment_method\"]].astype(\"int\"),\n            **rows[[\"user_subscriber\"]].astype(\"int\"),\n            **rows[[\"user_trialist\"]].astype(\"int\"),\n            **{\"previous_movie_ids\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_movie_ids\"].values,\n                                                              maxlen=seq_length, dtype='int32', value=0)},\n            **{\"previous_movie_years\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_movie_years\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=1980.0)},\n            **{\"previous_score\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_score\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=2.5)},\n            **{\"previous_days_since_last_rating\":\n                tf.keras.preprocessing.sequence.pad_sequences(rows[\"previous_days_since_last_rating\"].values,\n                                                              maxlen=seq_length, dtype='float32', value=0)}\n        )\n        return dict_features\n\n    try:\n        assert not tf.io.gfile.exists(output_path)\n    except AssertionError:\n        raise ValueError(\"Dataset already exists, load it. Remove timestamp from config in case of new run\")\n\n    sql_query = (\n        sql_query\n            .replace(\"__DATASET_ID__\", str(\"mubi_movie_data\"))\n            .replace(\"__FRAME_START__\", str(10))\n            .replace(\"__FRAME_END__\", str(1))\n    )\n\n    rows = load_dataset(sql_query)\n\n    dict_features = get_features_dict(rows)\n    del rows\n\n    save_tf_dataset(dict_features)\n\n"
            ],
            "image": "python:3.8"
          }
        },
        "exec-train-tensorflow-model": {
          "container": {
            "args": [
              "--type",
              "CustomJob",
              "--payload",
              "{\"display_name\": \"Train tensorflow model\", \"job_spec\": {\"worker_pool_specs\": [{\"machine_spec\": {\"machine_type\": \"n1-standard-64\", \"accelerator_type\": \"NVIDIA_TESLA_T4\", \"accelerator_count\": 4}, \"replica_count\": 1, \"container_spec\": {\"image_uri\": \"gcr.io/qwiklabs-gcp-04-424f1fdacc59/moviecust:0.1\", \"command\": [\"sh\", \"-c\", \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'tensorflow==2.6.2' 'tensorflow-recommenders==0.6.0' 'pandas==1.3.2' 'google-cloud-bigquery==2.34.2' 'pyarrow==7.0.0' 'google-cloud-storage==1.42.2' 'kfp==1.8.10' && \\\"$0\\\" \\\"$@\\\"\\n\", \"sh\", \"-ec\", \"program_path=$(mktemp -d)\\nprintf \\\"%s\\\" \\\"$0\\\" > \\\"$program_path/ephemeral_component.py\\\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \\\"$program_path/ephemeral_component.py\\\"                         \\\"$@\\\"\\n\", \"\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_tensorflow_model(\\n    data_root: str,\\n    movies_output_filename: str,\\n    train_output_filename: str,\\n    inference_output_filename: str,\\n    artifact_store: str,\\n    timestamp: str,\\n    bucket_name: str,\\n    model: Output[Model],\\n):\\n    import os\\n    import json\\n    import datetime\\n    import tensorflow as tf\\n    import tensorflow_recommenders as tfrs\\n    import time\\n    import subprocess\\n    import sys\\n    import tempfile\\n\\n    from google.cloud import storage\\n\\n    from mubireco.model.model_definition import create_model\\n    from mubireco.data.train import TrainDataset\\n    from mubireco.data.inference import InferenceDataset\\n\\n    from mubireco.utils.configuration import Configuration\\n\\n    batch_size = 10000\\n    num_evals = 100\\n    lr = 0.1\\n    timestamp_train = datetime.datetime.now().strftime(\\\"%Y%m%d%H%M%S\\\")\\n    embedding_dim = 32\\n    num_test_sample = 4\\n\\n    output_dir = os.path.join(f\\\"gs://{bucket_name}\\\", timestamp, timestamp_train)\\n\\n    config = Configuration(\\\"mubireco/config/config.yaml\\\")\\n\\n    ds_train = TrainDataset(config).read_tf_dataset()\\n    ds_inf = InferenceDataset(config).read_tf_dataset()\\n\\n    num_train_sample = len(ds_train) - num_test_sample\\n\\n    tf.random.set_seed(42)\\n    ds_train = ds_train.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\\n\\n    ds_train_shuffled = ds_train.take(num_train_sample)\\n    ds_val_shuffled = ds_train.skip(num_train_sample)\\n\\n    cached_train = ds_train_shuffled.batch(batch_size).repeat().cache()\\n    cached_val = ds_val_shuffled.batch(batch_size).cache()\\n\\n    steps_per_epoch = len(ds_train) // (batch_size * num_evals)\\n\\n    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\\n        os.path.join(output_dir, \\\"checkpoints\\\"), save_weights_only=True, verbose=1\\n    )\\n    tensorboard_cb = tf.keras.callbacks.TensorBoard(\\n        os.path.join(output_dir, \\\"tensorboard\\\"), histogram_freq=1\\n    )\\n\\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\\n                                                         monitor=\\\"factorized_top_k/top_100_categorical_accuracy\\\")\\n\\n    # For MirroredStrategy #\\n    strategy = tf.distribute.MirroredStrategy()\\n\\n    with strategy.scope():\\n        # End #\\n        model = create_model(batch_size, embedding_dim, config)\\n        model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=lr))\\n\\n    options = tf.data.Options()\\n    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\\n    cached_train = cached_train.with_options(options)\\n    cached_val = cached_val.with_options(options)\\n\\n    start = time.time()\\n    history = model.fit(\\n        cached_train,\\n        # validation_data=cached_val,\\n        steps_per_epoch=steps_per_epoch,\\n        epochs=num_evals,\\n        verbose=1,  # 0=silent, 1=progress bar, 2=one line per epoch\\n        callbacks=[checkpoint_cb, tensorboard_cb]  # , early_stopping_cb],\\n    )\\n    print(\\\"Training time with single GPUs: {}\\\".format(time.time() - start))\\n\\n    results = model.evaluate(cached_val, return_dict=True)\\n\\n    # Instantiates a client\\n    storage_client = storage.Client()\\n    bucket = storage_client.bucket(bucket_name)\\n    gcs_model_path = os.path.join(timestamp, timestamp_train, \\\"eval_results.json\\\")\\n    blob = bucket.blob(gcs_model_path)\\n    blob.upload_from_string(data=json.dumps(results), content_type='application/json')\\n    print(f\\\"Saved model in: {gcs_model_path}\\\")\\n\\n    # model.path = os.path.join(timestamp, timestamp_train)\\n    # model.save(model.path, save_format=\\\"tf\\\")\\n\\n    index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\\n\\n    index.index_from_dataset(\\n        tf.data.Dataset.zip(\\n            (ds_inf.map(lambda x: x[\\\"user_id\\\"]).batch(batch_size),\\n             ds_inf.batch(batch_size).map(model.candidate_model)))\\n    )\\n\\n    # example, to keep otherwise bug in saved model\\n    _, _ = index(tf.constant([42]))\\n\\n    with tempfile.TemporaryDirectory() as tmp:\\n        # Save the index.\\n        tf.saved_model.save(index, os.path.join(output_dir, \\\"models\\\"))\\n\\n\"], \"args\": [\"--executor_input\", \"{{$.json_escape[1]}}\", \"--function_to_execute\", \"train_tensorflow_model\"]}, \"disk_spec\": {\"boot_disk_type\": \"pd-ssd\", \"boot_disk_size_gb\": 100}}], \"service_account\": \"{{$.inputs.parameters['service_account']}}\", \"network\": \"{{$.inputs.parameters['network']}}\", \"tensorboard\": \"{{$.inputs.parameters['tensorboard']}}\", \"base_output_directory\": {\"output_uri_prefix\": \"{{$.inputs.parameters['base_output_directory']}}\"}}}",
              "--project",
              "{{$.inputs.parameters['project']}}",
              "--location",
              "{{$.inputs.parameters['location']}}",
              "--gcp_resources",
              "{{$.outputs.parameters['gcp_resources'].output_file}}"
            ],
            "command": [
              "python3",
              "-u",
              "-m",
              "google_cloud_pipeline_components.container.experimental.gcp_launcher.launcher"
            ],
            "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:0.2.1"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "tensorflow-train-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "endpoint-create": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-endpoint-create"
            },
            "dependentTasks": [
              "model-upload"
            ],
            "inputs": {
              "parameters": {
                "description": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "display_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-create-endpoint-job"
                    }
                  }
                },
                "encryption_spec_key_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "labels": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "location": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "us-central1"
                    }
                  }
                },
                "network": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "project": {
                  "componentInputParameter": "project_id"
                }
              }
            },
            "taskInfo": {
              "name": "endpoint-create"
            }
          },
          "inference-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-inference-dataset"
            },
            "inputs": {
              "parameters": {
                "data_root": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/20220317114008/data"
                    }
                  }
                },
                "movies_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "inference_mubi.tfdataset"
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                },
                "seq_length": {
                  "componentInputParameter": "seq_length"
                }
              }
            },
            "taskInfo": {
              "name": "inference-dataset"
            }
          },
          "model-deploy": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-model-deploy"
            },
            "dependentTasks": [
              "endpoint-create",
              "model-upload"
            ],
            "inputs": {
              "artifacts": {
                "endpoint": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "endpoint",
                    "producerTask": "endpoint-create"
                  }
                },
                "model": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "model",
                    "producerTask": "model-upload"
                  }
                }
              },
              "parameters": {
                "automatic_resources_max_replica_count": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "automatic_resources_min_replica_count": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "dedicated_resources_accelerator_count": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "dedicated_resources_accelerator_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "dedicated_resources_machine_type": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "n1-standard-16"
                    }
                  }
                },
                "dedicated_resources_max_replica_count": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "1"
                    }
                  }
                },
                "dedicated_resources_min_replica_count": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "1"
                    }
                  }
                },
                "deployed_model_display_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "disable_container_logging": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "enable_access_logging": {
                  "runtimeValue": {
                    "constantValue": {
                      "intValue": "0"
                    }
                  }
                },
                "explanation_metadata": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "explanation_parameters": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "service_account": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "traffic_split": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "model-deploy"
            }
          },
          "model-upload": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-model-upload"
            },
            "dependentTasks": [
              "train-tensorflow-model"
            ],
            "inputs": {
              "parameters": {
                "artifact_uri": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/20220311195134/20220317091043/models"
                    }
                  }
                },
                "description": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "display_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-model-upload-job"
                    }
                  }
                },
                "encryption_spec_key_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "explanation_metadata": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "explanation_parameters": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "instance_schema_uri": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "labels": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{}"
                    }
                  }
                },
                "location": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "us-central1"
                    }
                  }
                },
                "parameters_schema_uri": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "prediction_schema_uri": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "serving_container_args": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "[]"
                    }
                  }
                },
                "serving_container_command": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "[]"
                    }
                  }
                },
                "serving_container_environment_variables": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "[]"
                    }
                  }
                },
                "serving_container_health_route": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "serving_container_image_uri": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest"
                    }
                  }
                },
                "serving_container_ports": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "[]"
                    }
                  }
                },
                "serving_container_predict_route": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "model-upload"
            }
          },
          "movies-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-movies-dataset"
            },
            "inputs": {
              "parameters": {
                "data_root": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/20220317114008/data"
                    }
                  }
                },
                "movies_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "movies_mubi.tfdataset"
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                }
              }
            },
            "taskInfo": {
              "name": "movies-dataset"
            }
          },
          "train-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-dataset"
            },
            "inputs": {
              "parameters": {
                "data_root": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/20220317114008/data"
                    }
                  }
                },
                "movies_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train_mubi.tfdataset"
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                },
                "seq_length": {
                  "componentInputParameter": "seq_length"
                }
              }
            },
            "taskInfo": {
              "name": "train-dataset"
            }
          },
          "train-tensorflow-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-tensorflow-model"
            },
            "dependentTasks": [
              "inference-dataset",
              "movies-dataset",
              "train-dataset"
            ],
            "inputs": {
              "parameters": {
                "artifact_store": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/"
                    }
                  }
                },
                "base_output_directory": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "bucket_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store"
                    }
                  }
                },
                "data_root": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "gs://{{$.inputs.parameters['pipelineparam--model_name']}}-kfp-artifact-store/20220317114008/data"
                    }
                  }
                },
                "inference_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "inference_mubi.tfdataset"
                    }
                  }
                },
                "location": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "us-central1"
                    }
                  }
                },
                "movies_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "movies_mubi.tfdataset"
                    }
                  }
                },
                "network": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "pipelineparam--model_name": {
                  "componentInputParameter": "model_name"
                },
                "project": {
                  "componentInputParameter": "project_id"
                },
                "service_account": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "tensorboard": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": ""
                    }
                  }
                },
                "timestamp": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "20220317114008"
                    }
                  }
                },
                "train_output_filename": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "train_mubi.tfdataset"
                    }
                  }
                }
              }
            },
            "taskInfo": {
              "name": "Vertex Training for TF model"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "model_name": {
            "type": "STRING"
          },
          "project_id": {
            "type": "STRING"
          },
          "seq_length": {
            "type": "INT"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.10"
  },
  "runtimeConfig": {}
}